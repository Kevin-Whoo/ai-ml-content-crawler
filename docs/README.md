# ğŸ¤– Enhanced AI/ML Content Crawler

A secure, undetectable web crawler for gathering the latest AI/ML research content with advanced anti-detection capabilities.

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the Crawler
```bash
python main.py
```

That's it! The crawler will automatically gather content from all AI/ML sources.

## ğŸ“Š What It Does

- **ğŸ” Crawls 9 major AI/ML sources**: Anthropic, OpenAI, Meta AI, GitHub, Hugging Face, Medium, Google Scholar, ArXiv, IEEE
- **ğŸ¯ Finds relevant content** about Multimodal LLMs, AI Agents, and Energy AI
- **ğŸ“ Generates a comprehensive report** in markdown format
- **ğŸ›¡ï¸ Uses advanced security** and anti-detection features

## ğŸ›¡ï¸ Enhanced Security Features

âœ… **Input validation & SSRF protection**  
âœ… **Anti-detection browser profiles**  
âœ… **Smart caching system**  
âœ… **Rate limiting awareness**  
âœ… **Centralized error handling**  
âœ… **Content validation**  

## âš™ï¸ Configuration (Optional)

### Basic Settings
```bash
# Slower but more stealthy
export CRAWLER_DELAY="4.0"

# GitHub token for higher API limits (recommended)
export GITHUB_TOKEN="your_github_token"

# Enable proxy rotation (optional)
export CRAWLER_ENABLE_PROXIES="true"
export CRAWLER_PROXIES="http://proxy1:8080,http://proxy2:8080"
```

### Custom Keywords
Edit `config.py` to modify:
- `multimodal_keywords` - Keywords for multimodal AI content
- `ai_agent_keywords` - Keywords for AI agent content  
- `energy_ai_keywords` - Keywords for energy AI content
- `max_days_back` - How far back to search (default: 90 days)
- `max_results_per_source` - Results per source (default: 25)

## ğŸ“ Output

The crawler creates:
- `output/AI_ML_Resources_YYYYMMDD_HHMMSS.md` - Main comprehensive report
- `cache/` directory - Cached requests for better performance

## ğŸ”§ Advanced Usage

### Proxy Setup (Optional)
Create `proxies.txt` with one proxy per line:
```
http://proxy1.example.com:8080
socks5://proxy2.example.com:1080
```

### Custom Configuration
Edit `config.py` directly for fine-tuning:
```python
# In config.py
request_delay: float = 2.0  # Seconds between requests
cache_ttl: int = 3600      # Cache time in seconds
max_results_per_source: int = 25
```

## ğŸ“Š Real-time Monitoring

The crawler shows live statistics:
```
ğŸš€ Starting comprehensive AI/ML content crawl...
ğŸ“¡ Initiating github crawler...
âœ… github: Found 25 relevant items
âœ… anthropic: Found 12 relevant items
ğŸ’¾ Cache stats: 78.4% hit rate, 156 entries
ğŸ‰ Crawling complete! Found 67 relevant items
```

## ğŸš¨ Troubleshooting

**Getting rate limited?**
```bash
export CRAWLER_DELAY="5.0"  # Increase delay
```

**Want to clear cache?**
```bash
rm -rf cache/
```

**Proxy not working?**
```bash
# Test your proxy first
curl --proxy http://your-proxy:8080 https://httpbin.org/ip
```

## ğŸ”’ Security & Ethics

- **Respects rate limits** and server resources
- **Only accesses public research content**
- **Follows robots.txt** and terms of service
- **Validates all inputs** to prevent security issues
- **Uses domain allowlist** to prevent SSRF attacks

## ğŸ“„ File Structure

```
Web_Crawling/
â”œâ”€â”€ main.py              # ğŸ¯ Main runner (use this!)
â”œâ”€â”€ config.py            # âš™ï¸ Configuration settings
â”œâ”€â”€ requirements.txt     # ğŸ“¦ Dependencies
â”œâ”€â”€ crawlers/           # ğŸ•·ï¸ Crawler implementations
â”œâ”€â”€ utils/              # ğŸ› ï¸ Security & utility modules
â”œâ”€â”€ output/             # ğŸ“Š Generated reports
â””â”€â”€ cache/              # ğŸ’¾ Request cache
```

## ğŸ¯ Just Remember

**To run the crawler:** `python main.py`

Everything else is automatic! ğŸ‰