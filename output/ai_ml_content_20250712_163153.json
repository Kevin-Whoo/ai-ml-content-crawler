{
  "metadata": {
    "crawl_timestamp": "2025-07-12T16:32:55.253826",
    "total_sources": 4,
    "total_items": 52,
    "config": {
      "max_results_per_source": 50,
      "max_days_back": 90
    }
  },
  "results": {
    "anthropic": [
      {
        "title": "Press inquiries",
        "url": "https://www.anthropic.com/news/claude-4",
        "date": "",
        "summary": "",
        "content": "",
        "tags": [
          "anthropic",
          "blog"
        ],
        "source": "Anthropic Blog",
        "crawled_at": "2025-07-12T16:31:55.997908",
        "relevance_score": 1.0,
        "relevance_reasons": []
      },
      {
        "title": "Advancing Claude for Education",
        "url": "https://www.anthropic.com/news/advancing-claude-for-education",
        "date": "",
        "summary": "",
        "content": "",
        "tags": [
          "anthropic",
          "blog"
        ],
        "source": "Anthropic Blog",
        "crawled_at": "2025-07-12T16:31:56.012601",
        "relevance_score": 1.0,
        "relevance_reasons": []
      },
      {
        "title": "Advancing Claude for Education",
        "url": "https://www.anthropic.com/news/advancing-claude-for-education",
        "date": "",
        "summary": "",
        "content": "",
        "tags": [
          "anthropic",
          "blog"
        ],
        "source": "Anthropic Blog",
        "crawled_at": "2025-07-12T16:31:56.026994",
        "relevance_score": 1.0,
        "relevance_reasons": []
      }
    ],
    "openai": [],
    "meta": [],
    "github": [
      {
        "title": "Blaizzy/mlx-vlm",
        "url": "https://github.com/Blaizzy/mlx-vlm",
        "date": "2025-07-11T23:49:22Z",
        "summary": "MLX-VLM is a package for inference and fine-tuning of Vision Language Models (VLMs) on your Mac using MLX.\n\n‚≠ê 1481 stars | üç¥ 146 forks | üìù Python",
        "content": "MLX-VLM is a package for inference and fine-tuning of Vision Language Models (VLMs) on your Mac using MLX.",
        "tags": [
          "github",
          "repository",
          "apple-silicon",
          "florence2",
          "idefics",
          "llava",
          "llm",
          "local-ai",
          "mlx",
          "molmo",
          "paligemma",
          "pixtral",
          "vision-framework",
          "vision-language-model",
          "vision-transformer",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962673",
        "stars": 1481,
        "forks": 146,
        "language": "Python",
        "topics": [
          "apple-silicon",
          "florence2",
          "idefics",
          "llava",
          "llm",
          "local-ai",
          "mlx",
          "molmo",
          "paligemma",
          "pixtral",
          "vision-framework",
          "vision-language-model",
          "vision-transformer"
        ],
        "owner": "Blaizzy",
        "created_at": "2024-04-16T15:10:12Z",
        "relevance_score": 16.481,
        "relevance_reasons": [
          "Multimodal AI: vision language model, llava, vlm",
          "Key Tech: llava, llm, transformer",
          "High engagement (1481 stars)"
        ]
      },
      {
        "title": "shibing624/agentica",
        "url": "https://github.com/shibing624/agentica",
        "date": "2025-07-10T16:55:58Z",
        "summary": "Agentica: Effortlessly Build Intelligent, Reflective, and Collaborative Multimodal AI Agents! ÊûÑÂª∫Êô∫ËÉΩÁöÑÂ§öÊ®°ÊÄÅAI Agent„ÄÇ\n\n‚≠ê 184 stars | üç¥ 22 forks | üìù Python",
        "content": "Agentica: Effortlessly Build Intelligent, Reflective, and Collaborative Multimodal AI Agents! ÊûÑÂª∫Êô∫ËÉΩÁöÑÂ§öÊ®°ÊÄÅAI Agent„ÄÇ",
        "tags": [
          "github",
          "repository",
          "actionflow",
          "agent",
          "agentica",
          "agents",
          "langchain",
          "llm",
          "multi-agent",
          "workflows",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911366",
        "stars": 184,
        "forks": 22,
        "language": "Python",
        "topics": [
          "actionflow",
          "agent",
          "agentica",
          "agents",
          "langchain",
          "llm",
          "multi-agent",
          "workflows"
        ],
        "owner": "shibing624",
        "created_at": "2024-06-03T12:07:51Z",
        "relevance_score": 15.404,
        "relevance_reasons": [
          "Multimodal AI: multimodal ai, multimodal",
          "AI Agents: multi-agent, ai agent, agent workflow",
          "Key Tech: llm, langchain"
        ]
      },
      {
        "title": "sgl-project/sglang",
        "url": "https://github.com/sgl-project/sglang",
        "date": "2025-07-12T08:22:25Z",
        "summary": "SGLang is a fast serving framework for large language models and vision language models.\n\n‚≠ê 15933 stars | üç¥ 2322 forks | üìù Python",
        "content": "SGLang is a fast serving framework for large language models and vision language models.",
        "tags": [
          "github",
          "repository",
          "blackwell",
          "cuda",
          "deepseek",
          "deepseek-llm",
          "deepseek-r1",
          "deepseek-v3",
          "inference",
          "llama",
          "llama3",
          "llama4",
          "llama5",
          "llava",
          "llm",
          "llm-serving",
          "moe",
          "openai",
          "pytorch",
          "qwen3",
          "transformer",
          "vlm",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962557",
        "stars": 15933,
        "forks": 2322,
        "language": "Python",
        "topics": [
          "blackwell",
          "cuda",
          "deepseek",
          "deepseek-llm",
          "deepseek-r1",
          "deepseek-v3",
          "inference",
          "llama",
          "llama3",
          "llama4",
          "llama5",
          "llava",
          "llm",
          "llm-serving",
          "moe",
          "openai",
          "pytorch",
          "qwen3",
          "transformer",
          "vlm"
        ],
        "owner": "sgl-project",
        "created_at": "2024-01-08T04:15:52Z",
        "relevance_score": 14.5,
        "relevance_reasons": [
          "Multimodal AI: vision language model, llava, vlm",
          "Key Tech: llava, llm, transformer",
          "High engagement (15933 stars)"
        ]
      },
      {
        "title": "SkyworkAI/Skywork-R1V",
        "url": "https://github.com/SkyworkAI/Skywork-R1V",
        "date": "2025-07-12T07:12:41Z",
        "summary": "Skywork-R1V is an advanced multimodal AI model series developed by Skywork AI (Kunlun Inc.), specializing in vision-language reasoning.\n\n‚≠ê 2719 stars | üç¥ 254 forks | üìù Python",
        "content": "Skywork-R1V is an advanced multimodal AI model series developed by Skywork AI (Kunlun Inc.), specializing in vision-language reasoning.",
        "tags": [
          "github",
          "repository",
          "deepseek-r1",
          "grpo",
          "llm",
          "multimodal-r1",
          "multimodal-understanding",
          "r1v",
          "reasoning",
          "reinforcement-learning",
          "skywork-r1v",
          "vlm",
          "vlm-r1",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911165",
        "stars": 2719,
        "forks": 254,
        "language": "Python",
        "topics": [
          "deepseek-r1",
          "grpo",
          "llm",
          "multimodal-r1",
          "multimodal-understanding",
          "r1v",
          "reasoning",
          "reinforcement-learning",
          "skywork-r1v",
          "vlm",
          "vlm-r1"
        ],
        "owner": "SkyworkAI",
        "created_at": "2025-03-15T08:11:44Z",
        "relevance_score": 12.5,
        "relevance_reasons": [
          "Multimodal AI: vlm, multimodal ai, vision-language",
          "Key Tech: llm",
          "High engagement (2719 stars)"
        ]
      },
      {
        "title": "modelscope/ms-swift",
        "url": "https://github.com/modelscope/ms-swift",
        "date": "2025-07-12T06:34:17Z",
        "summary": "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).\n\n‚≠ê 8620 stars | üç¥ 743 forks | üìù Python",
        "content": "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).",
        "tags": [
          "github",
          "repository",
          "deepseek-r1",
          "deploy",
          "embedding",
          "grpo",
          "internvl",
          "liger",
          "llama",
          "llama4",
          "llm",
          "lora",
          "megatron",
          "multimodal",
          "omni",
          "open-r1",
          "peft",
          "qwen2-vl",
          "qwen3",
          "qwen3-moe",
          "rft",
          "sft",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910818",
        "stars": 8620,
        "forks": 743,
        "language": "Python",
        "topics": [
          "deepseek-r1",
          "deploy",
          "embedding",
          "grpo",
          "internvl",
          "liger",
          "llama",
          "llama4",
          "llm",
          "lora",
          "megatron",
          "multimodal",
          "omni",
          "open-r1",
          "peft",
          "qwen2-vl",
          "qwen3",
          "qwen3-moe",
          "rft",
          "sft"
        ],
        "owner": "modelscope",
        "created_at": "2023-08-01T15:06:39Z",
        "relevance_score": 10.0,
        "relevance_reasons": [
          "Multimodal AI: llava, multimodal",
          "Key Tech: llava, llm",
          "High engagement (8620 stars)"
        ]
      },
      {
        "title": "huggingface/transformers",
        "url": "https://github.com/huggingface/transformers",
        "date": "2025-07-12T08:05:08Z",
        "summary": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. \n\n‚≠ê 146829 stars | üç¥ 29617 forks | üìù Python",
        "content": "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.",
        "tags": [
          "github",
          "repository",
          "audio",
          "deep-learning",
          "deepseek",
          "gemma",
          "glm",
          "hacktoberfest",
          "llm",
          "machine-learning",
          "model-hub",
          "natural-language-processing",
          "nlp",
          "pretrained-models",
          "python",
          "pytorch",
          "pytorch-transformers",
          "qwen",
          "speech-recognition",
          "transformer",
          "vlm",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911009",
        "stars": 146829,
        "forks": 29617,
        "language": "Python",
        "topics": [
          "audio",
          "deep-learning",
          "deepseek",
          "gemma",
          "glm",
          "hacktoberfest",
          "llm",
          "machine-learning",
          "model-hub",
          "natural-language-processing",
          "nlp",
          "pretrained-models",
          "python",
          "pytorch",
          "pytorch-transformers",
          "qwen",
          "speech-recognition",
          "transformer",
          "vlm"
        ],
        "owner": "huggingface",
        "created_at": "2018-10-29T13:56:00Z",
        "relevance_score": 10.0,
        "relevance_reasons": [
          "Multimodal AI: vlm, multimodal",
          "Key Tech: llm, transformer",
          "High engagement (146829 stars)"
        ]
      },
      {
        "title": "PRITHIVSAKTHIUR/Multimodal-VLMs",
        "url": "https://github.com/PRITHIVSAKTHIUR/Multimodal-VLMs",
        "date": "2025-07-12T05:17:47Z",
        "summary": "A comprehensive Gradio-based interface for running multiple state-of-the-art Vision-Language Models (VLMs) for Optical Character Recognition (OCR) and Visual Question Answering (VQA) tasks.\n\n‚≠ê 2 stars | üç¥ 0 forks | üìù Python",
        "content": "A comprehensive Gradio-based interface for running multiple state-of-the-art Vision-Language Models (VLMs) for Optical Character Recognition (OCR) and Visual Question Answering (VQA) tasks.",
        "tags": [
          "github",
          "repository",
          "gradio",
          "huggingface-transformers",
          "multimodal-large-language-models",
          "multimodality",
          "ocr",
          "pillow",
          "qwen2-5-vl",
          "torch",
          "torchvision",
          "vision-transformer",
          "vlms",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962575",
        "stars": 2,
        "forks": 0,
        "language": "Python",
        "topics": [
          "gradio",
          "huggingface-transformers",
          "multimodal-large-language-models",
          "multimodality",
          "ocr",
          "pillow",
          "qwen2-5-vl",
          "torch",
          "torchvision",
          "vision-transformer",
          "vlms"
        ],
        "owner": "PRITHIVSAKTHIUR",
        "created_at": "2025-07-07T02:58:34Z",
        "relevance_score": 9.502,
        "relevance_reasons": [
          "Multimodal AI: visual question answering, vlm, vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "xlang-ai/OSWorld",
        "url": "https://github.com/xlang-ai/OSWorld",
        "date": "2025-07-11T02:19:26Z",
        "summary": "[NeurIPS 2024] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\n\n‚≠ê 1968 stars | üç¥ 252 forks | üìù Python",
        "content": "[NeurIPS 2024] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "tags": [
          "github",
          "repository",
          "agent",
          "artificial-intelligence",
          "benchmark",
          "cli",
          "code-generation",
          "gui",
          "language-model",
          "large-action-model",
          "llm",
          "multimodal",
          "natural-language-processing",
          "reinforcement-learning",
          "rpa",
          "vlm",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911216",
        "stars": 1968,
        "forks": 252,
        "language": "Python",
        "topics": [
          "agent",
          "artificial-intelligence",
          "benchmark",
          "cli",
          "code-generation",
          "gui",
          "language-model",
          "large-action-model",
          "llm",
          "multimodal",
          "natural-language-processing",
          "reinforcement-learning",
          "rpa",
          "vlm"
        ],
        "owner": "xlang-ai",
        "created_at": "2023-10-16T01:49:13Z",
        "relevance_score": 8.468,
        "relevance_reasons": [
          "Multimodal AI: vlm, multimodal",
          "Key Tech: llm",
          "High engagement (1968 stars)"
        ]
      },
      {
        "title": "Flagro/OmniModKit",
        "url": "https://github.com/Flagro/OmniModKit",
        "date": "2025-07-11T16:56:05Z",
        "summary": "Multimodal LLM toolkit\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Multimodal LLM toolkit",
        "tags": [
          "github",
          "repository",
          "langchain",
          "llm",
          "multimodal",
          "openai",
          "toolkit",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911062",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "langchain",
          "llm",
          "multimodal",
          "openai",
          "toolkit"
        ],
        "owner": "Flagro",
        "created_at": "2024-12-15T12:07:01Z",
        "relevance_score": 8.0,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "AI Agents: langchain",
          "Key Tech: llm, langchain"
        ]
      },
      {
        "title": "PRITHIVSAKTHIUR/Multimodal-OCR",
        "url": "https://github.com/PRITHIVSAKTHIUR/Multimodal-OCR",
        "date": "2025-07-12T05:12:32Z",
        "summary": "Vision Language Model : tailored for tasks that involve [messy] optical character recognition (ocr), image-to-text conversion, and math problem solving with latex formatting.\n\n‚≠ê 4 stars | üç¥ 0 forks | üìù Python",
        "content": "Vision Language Model : tailored for tasks that involve [messy] optical character recognition (ocr), image-to-text conversion, and math problem solving with latex formatting.",
        "tags": [
          "github",
          "repository",
          "huggingface-transformers",
          "monkey-ocr",
          "ocr-python",
          "ocr-recognition",
          "opencv-python",
          "pillow",
          "qwen2-5-vl",
          "qwen2-vl-2b",
          "video-processing",
          "video-understanding",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962592",
        "stars": 4,
        "forks": 0,
        "language": "Python",
        "topics": [
          "huggingface-transformers",
          "monkey-ocr",
          "ocr-python",
          "ocr-recognition",
          "opencv-python",
          "pillow",
          "qwen2-5-vl",
          "qwen2-vl-2b",
          "video-processing",
          "video-understanding"
        ],
        "owner": "PRITHIVSAKTHIUR",
        "created_at": "2025-01-28T18:29:13Z",
        "relevance_score": 7.504,
        "relevance_reasons": [
          "Multimodal AI: vision language model, image-to-text, multimodal",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "ahnjaewoo/MAC",
        "url": "https://github.com/ahnjaewoo/MAC",
        "date": "2025-07-11T00:08:47Z",
        "summary": "Code for our ACL 2025 Main paper: \"Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates\"\n\n‚≠ê 3 stars | üç¥ 0 forks | üìù Python",
        "content": "Code for our ACL 2025 Main paper: \"Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates\"",
        "tags": [
          "github",
          "repository",
          "acl2025",
          "adversarial-attacks",
          "benchmark",
          "llm",
          "multimodality",
          "vision-language-compositionality",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911199",
        "stars": 3,
        "forks": 0,
        "language": "Python",
        "topics": [
          "acl2025",
          "adversarial-attacks",
          "benchmark",
          "llm",
          "multimodality",
          "vision-language-compositionality"
        ],
        "owner": "ahnjaewoo",
        "created_at": "2025-05-24T23:00:34Z",
        "relevance_score": 7.503,
        "relevance_reasons": [
          "Multimodal AI: clip, vision-language, multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "xorbitsai/inference",
        "url": "https://github.com/xorbitsai/inference",
        "date": "2025-07-12T06:24:19Z",
        "summary": "Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\n\n‚≠ê 8208 stars | üç¥ 706 forks | üìù Python",
        "content": "Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.",
        "tags": [
          "github",
          "repository",
          "artificial-intelligence",
          "chatglm",
          "deployment",
          "flan-t5",
          "gemma",
          "ggml",
          "glm4",
          "inference",
          "llama",
          "llama3",
          "llamacpp",
          "llm",
          "machine-learning",
          "mistral",
          "openai-api",
          "pytorch",
          "qwen",
          "vllm",
          "whisper",
          "wizardlm",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911428",
        "stars": 8208,
        "forks": 706,
        "language": "Python",
        "topics": [
          "artificial-intelligence",
          "chatglm",
          "deployment",
          "flan-t5",
          "gemma",
          "ggml",
          "glm4",
          "inference",
          "llama",
          "llama3",
          "llamacpp",
          "llm",
          "machine-learning",
          "mistral",
          "openai-api",
          "pytorch",
          "qwen",
          "vllm",
          "whisper",
          "wizardlm"
        ],
        "owner": "xorbitsai",
        "created_at": "2023-06-14T07:05:04Z",
        "relevance_score": 7.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm",
          "High engagement (8208 stars)"
        ]
      },
      {
        "title": "FrancescoRomeo02/multimodalragApp",
        "url": "https://github.com/FrancescoRomeo02/multimodalragApp",
        "date": "2025-07-11T17:35:04Z",
        "summary": "Advanced multimodal RAG system for querying PDF documents with text, images, and tables using vector embeddings, semantic chunking, and LLMs via Groq API\n\n‚≠ê 0 stars | üç¥ 1 forks | üìù Python",
        "content": "Advanced multimodal RAG system for querying PDF documents with text, images, and tables using vector embeddings, semantic chunking, and LLMs via Groq API",
        "tags": [
          "github",
          "repository",
          "ai",
          "chatbot",
          "computer-vision",
          "document-intelligence",
          "groq",
          "langchain",
          "machine-learning",
          "multimodal",
          "nlp",
          "pdf-analysis",
          "qdrant",
          "rag",
          "semantic-search",
          "streamlit",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911045",
        "stars": 0,
        "forks": 1,
        "language": "Python",
        "topics": [
          "ai",
          "chatbot",
          "computer-vision",
          "document-intelligence",
          "groq",
          "langchain",
          "machine-learning",
          "multimodal",
          "nlp",
          "pdf-analysis",
          "qdrant",
          "rag",
          "semantic-search",
          "streamlit"
        ],
        "owner": "FrancescoRomeo02",
        "created_at": "2025-06-15T19:04:47Z",
        "relevance_score": 7.01,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "AI Agents: langchain",
          "Key Tech: llm, langchain"
        ]
      },
      {
        "title": "ustcfd/PoseLLaVA",
        "url": "https://github.com/ustcfd/PoseLLaVA",
        "date": "2025-07-09T07:18:51Z",
        "summary": "[AAAI 2025] PoseLLaVA: Pose Centric Multimodal LLM for Fine-Grained 3D Pose Manipulation\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "[AAAI 2025] PoseLLaVA: Pose Centric Multimodal LLM for Fine-Grained 3D Pose Manipulation",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911448",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "ustcfd",
        "created_at": "2024-12-12T07:46:31Z",
        "relevance_score": 7.0,
        "relevance_reasons": [
          "Multimodal AI: llava, multimodal",
          "Key Tech: llava, llm"
        ]
      },
      {
        "title": "stanford-crfm/helm",
        "url": "https://github.com/stanford-crfm/helm",
        "date": "2025-07-12T03:35:08Z",
        "summary": "Holistic Evaluation of Language Models (HELM) is an open source Python framework created by the Center for Research on Foundation Models (CRFM) at Stanford for holistic, reproducible and transparent evaluation of foundation models, including large language models (LLMs) and multimodal models.\n\n‚≠ê 2334 stars | üç¥ 311 forks | üìù Python",
        "content": "Holistic Evaluation of Language Models (HELM) is an open source Python framework created by the Center for Research on Foundation Models (CRFM) at Stanford for holistic, reproducible and transparent evaluation of foundation models, including large language models (LLMs) and multimodal models.",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910899",
        "stars": 2334,
        "forks": 311,
        "language": "Python",
        "topics": [],
        "owner": "stanford-crfm",
        "created_at": "2021-11-29T08:53:17Z",
        "relevance_score": 6.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm",
          "High engagement (2334 stars)"
        ]
      },
      {
        "title": "iterative/datachain",
        "url": "https://github.com/iterative/datachain",
        "date": "2025-07-12T02:27:16Z",
        "summary": "ETL, Analytics, Versioning for Unstructured Data\n\n‚≠ê 2603 stars | üç¥ 116 forks | üìù Python",
        "content": "ETL, Analytics, Versioning for Unstructured Data",
        "tags": [
          "github",
          "repository",
          "ai",
          "cv",
          "data-analytics",
          "data-wrangling",
          "embeddings",
          "llm",
          "llm-eval",
          "machine-learning",
          "mlops",
          "multimodal",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910921",
        "stars": 2603,
        "forks": 116,
        "language": "Python",
        "topics": [
          "ai",
          "cv",
          "data-analytics",
          "data-wrangling",
          "embeddings",
          "llm",
          "llm-eval",
          "machine-learning",
          "mlops",
          "multimodal"
        ],
        "owner": "iterative",
        "created_at": "2024-06-25T22:29:35Z",
        "relevance_score": 6.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm",
          "High engagement (2603 stars)"
        ]
      },
      {
        "title": "bentoml/BentoML",
        "url": "https://github.com/bentoml/BentoML",
        "date": "2025-07-12T05:34:10Z",
        "summary": "The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!\n\n‚≠ê 7889 stars | üç¥ 855 forks | üìù Python",
        "content": "The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!",
        "tags": [
          "github",
          "repository",
          "ai-inference",
          "deep-learning",
          "generative-ai",
          "inference-platform",
          "llm",
          "llm-inference",
          "llm-serving",
          "llmops",
          "machine-learning",
          "ml-engineering",
          "mlops",
          "model-inference-service",
          "model-serving",
          "multimodal",
          "python",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911384",
        "stars": 7889,
        "forks": 855,
        "language": "Python",
        "topics": [
          "ai-inference",
          "deep-learning",
          "generative-ai",
          "inference-platform",
          "llm",
          "llm-inference",
          "llm-serving",
          "llmops",
          "machine-learning",
          "ml-engineering",
          "mlops",
          "model-inference-service",
          "model-serving",
          "multimodal",
          "python"
        ],
        "owner": "bentoml",
        "created_at": "2019-04-02T01:39:27Z",
        "relevance_score": 6.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm",
          "High engagement (7889 stars)"
        ]
      },
      {
        "title": "Sanyi54/Clipart-126-DomainNet",
        "url": "https://github.com/Sanyi54/Clipart-126-DomainNet",
        "date": "2025-07-12T07:30:59Z",
        "summary": "Clipart-126-DomainNet is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to classify clipart images into 126 domain categories using the SiglipForImageClassification architecture\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Clipart-126-DomainNet is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to classify clipart images into 126 domain categories using the SiglipForImageClassification architecture",
        "tags": [
          "github",
          "repository",
          "art",
          "classification",
          "demo-app",
          "gradio",
          "huggingface-spaces",
          "huggingface-transformers",
          "image-classification",
          "llama",
          "siglip2",
          "torchvision",
          "vision-transformer",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962513",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "art",
          "classification",
          "demo-app",
          "gradio",
          "huggingface-spaces",
          "huggingface-transformers",
          "image-classification",
          "llama",
          "siglip2",
          "torchvision",
          "vision-transformer"
        ],
        "owner": "Sanyi54",
        "created_at": "2025-03-30T06:56:03Z",
        "relevance_score": 6.5,
        "relevance_reasons": [
          "Multimodal AI: clip, vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "AsherJingkongChen/vlm-bridge-for-image-captioning",
        "url": "https://github.com/AsherJingkongChen/vlm-bridge-for-image-captioning",
        "date": "2025-07-11T21:49:55Z",
        "summary": "Vision-Language Bridge for Image Captioning - A modular system using Cross-Attention Bridge Module for connecting frozen DINOv2 and Gemma-2-2B models\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Vision-Language Bridge for Image Captioning - A modular system using Cross-Attention Bridge Module for connecting frozen DINOv2 and Gemma-2-2B models",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962690",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "AsherJingkongChen",
        "created_at": "2025-07-10T12:24:05Z",
        "relevance_score": 6.0,
        "relevance_reasons": [
          "Multimodal AI: vlm, vision-language, image captioning"
        ]
      },
      {
        "title": "autogluon/autogluon-assistant",
        "url": "https://github.com/autogluon/autogluon-assistant",
        "date": "2025-07-11T20:06:42Z",
        "summary": "Multi-Agent System Powered by LLMs for End-to-end Multimodal ML Automation\n\n‚≠ê 142 stars | üç¥ 29 forks | üìù Python",
        "content": "Multi-Agent System Powered by LLMs for End-to-end Multimodal ML Automation",
        "tags": [
          "github",
          "repository",
          "automl",
          "data-science",
          "llm",
          "machine-learning",
          "mlzero",
          "multi-agent",
          "multimodal",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911027",
        "stars": 142,
        "forks": 29,
        "language": "Python",
        "topics": [
          "automl",
          "data-science",
          "llm",
          "machine-learning",
          "mlzero",
          "multi-agent",
          "multimodal"
        ],
        "owner": "autogluon",
        "created_at": "2024-04-29T07:23:46Z",
        "relevance_score": 5.932,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "AI Agents: multi-agent",
          "Key Tech: llm"
        ]
      },
      {
        "title": "fblissjr/heylookitsanllm",
        "url": "https://github.com/fblissjr/heylookitsanllm",
        "date": "2025-07-10T17:25:04Z",
        "summary": "unified on-device / edge multimodal inference for mlx and gguf / llama.cpp multimodal LLMs and VLMs\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "unified on-device / edge multimodal inference for mlx and gguf / llama.cpp multimodal LLMs and VLMs",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911257",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "fblissjr",
        "created_at": "2025-06-30T17:59:51Z",
        "relevance_score": 5.5,
        "relevance_reasons": [
          "Multimodal AI: vlm, multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "codewithdark-git/EVLMs",
        "url": "https://github.com/codewithdark-git/EVLMs",
        "date": "2025-07-12T08:29:07Z",
        "summary": "EVLMs is a framework for building explainable vision-language models specifically designed for medical image analysis\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "EVLMs is a framework for building explainable vision-language models specifically designed for medical image analysis",
        "tags": [
          "github",
          "repository",
          "ai",
          "cuda",
          "dnn",
          "language-model",
          "llm",
          "lvlms",
          "medical-imaging",
          "ml",
          "nn",
          "python",
          "research-project",
          "vllm",
          "vllms",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962299",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "ai",
          "cuda",
          "dnn",
          "language-model",
          "llm",
          "lvlms",
          "medical-imaging",
          "ml",
          "nn",
          "python",
          "research-project",
          "vllm",
          "vllms"
        ],
        "owner": "codewithdark-git",
        "created_at": "2025-05-24T17:09:14Z",
        "relevance_score": 5.5,
        "relevance_reasons": [
          "Multimodal AI: vlm, vision-language",
          "Key Tech: llm"
        ]
      },
      {
        "title": "video-db/videodb-python",
        "url": "https://github.com/video-db/videodb-python",
        "date": "2025-07-10T11:54:56Z",
        "summary": "VideoDB Python SDK\n\n‚≠ê 74 stars | üç¥ 11 forks | üìù Python",
        "content": "VideoDB Python SDK",
        "tags": [
          "github",
          "repository",
          "agent",
          "ai",
          "copilot",
          "database",
          "gpt",
          "infrastructure",
          "llm",
          "multimedia",
          "multimodal",
          "python",
          "rag",
          "sdk",
          "video",
          "video-processing",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911273",
        "stars": 74,
        "forks": 11,
        "language": "Python",
        "topics": [
          "agent",
          "ai",
          "copilot",
          "database",
          "gpt",
          "infrastructure",
          "llm",
          "multimedia",
          "multimodal",
          "python",
          "rag",
          "sdk",
          "video",
          "video-processing"
        ],
        "owner": "video-db",
        "created_at": "2023-12-18T15:20:04Z",
        "relevance_score": 5.184,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm, copilot"
        ]
      },
      {
        "title": "agosalvez/tfm-lora-specialized-adapter",
        "url": "https://github.com/agosalvez/tfm-lora-specialized-adapter",
        "date": "2025-07-10T11:12:44Z",
        "summary": "TFM: Sistema de especializaci√≥n de LLMs mediante adapters LoRA. Procesamiento multimodal de documentos acad√©micos y entrenamiento eficiente sobre PHI4-mini-instruct con recursos limitados.Tags: machine-learning, fine-tuning, lora, peft, llm, phi4, multimodal, tfm, specialized-adapter, llamafactory\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "TFM: Sistema de especializaci√≥n de LLMs mediante adapters LoRA. Procesamiento multimodal de documentos acad√©micos y entrenamiento eficiente sobre PHI4-mini-instruct con recursos limitados.Tags: machine-learning, fine-tuning, lora, peft, llm, phi4, multimodal, tfm, specialized-adapter, llamafactory",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911292",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "agosalvez",
        "created_at": "2025-06-01T13:07:37Z",
        "relevance_score": 5.0,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm, fine-tuning"
        ]
      },
      {
        "title": "pixeltable/pixeltable",
        "url": "https://github.com/pixeltable/pixeltable",
        "date": "2025-07-12T07:21:53Z",
        "summary": "Pixeltable ‚Äî AI Data infrastructure providing a declarative, incremental approach for multimodal workloads.\n\n‚≠ê 584 stars | üç¥ 78 forks | üìù Python",
        "content": "Pixeltable ‚Äî AI Data infrastructure providing a declarative, incremental approach for multimodal workloads.",
        "tags": [
          "github",
          "repository",
          "ai",
          "artificial-intelligence",
          "chatbot",
          "computer-vision",
          "data-science",
          "database",
          "feature-engineering",
          "feature-store",
          "genai",
          "llm",
          "machine-learning",
          "ml",
          "mlops",
          "multimodal",
          "vector-database",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910971",
        "stars": 584,
        "forks": 78,
        "language": "Python",
        "topics": [
          "ai",
          "artificial-intelligence",
          "chatbot",
          "computer-vision",
          "data-science",
          "database",
          "feature-engineering",
          "feature-store",
          "genai",
          "llm",
          "machine-learning",
          "ml",
          "mlops",
          "multimodal",
          "vector-database"
        ],
        "owner": "pixeltable",
        "created_at": "2023-05-10T18:03:02Z",
        "relevance_score": 4.864,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm",
          "High engagement (584 stars)"
        ]
      },
      {
        "title": "jesus3476/Fire-Detection-Siglip2",
        "url": "https://github.com/jesus3476/Fire-Detection-Siglip2",
        "date": "2025-07-12T07:34:19Z",
        "summary": "Fire-Detection-Siglip2 is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to detect fire, smoke, or normal conditions using the SiglipForImageClassification architecture. \n\n‚≠ê 3 stars | üç¥ 1 forks | üìù Python",
        "content": "Fire-Detection-Siglip2 is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to detect fire, smoke, or normal conditions using the SiglipForImageClassification architecture.",
        "tags": [
          "github",
          "repository",
          "fire-detection",
          "google",
          "huggingface",
          "huggingface-transformers",
          "image-classification",
          "llama",
          "normal",
          "siglip",
          "siglip2",
          "smoke",
          "vit",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962476",
        "stars": 3,
        "forks": 1,
        "language": "Python",
        "topics": [
          "fire-detection",
          "google",
          "huggingface",
          "huggingface-transformers",
          "image-classification",
          "llama",
          "normal",
          "siglip",
          "siglip2",
          "smoke",
          "vit"
        ],
        "owner": "jesus3476",
        "created_at": "2025-04-03T17:42:59Z",
        "relevance_score": 4.513,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "pedropqv/nsfw-image-detection",
        "url": "https://github.com/pedropqv/nsfw-image-detection",
        "date": "2025-07-12T08:03:10Z",
        "summary": "nsfw-image-detection is a vision-language encoder model fine-tuned from siglip2-base-patch16-256 for multi-class image classification. Built on the SiglipForImageClassification architecture, the model is trained to identify and categorize content types in images, especially for explicit, suggestive, or safe media filtering.\n\n‚≠ê 1 stars | üç¥ 0 forks | üìù Python",
        "content": "nsfw-image-detection is a vision-language encoder model fine-tuned from siglip2-base-patch16-256 for multi-class image classification. Built on the SiglipForImageClassification architecture, the model is trained to identify and categorize content types in images, especially for explicit, suggestive, or safe media filtering.",
        "tags": [
          "github",
          "repository",
          "ai",
          "echarts",
          "google",
          "gradio",
          "learning-with-noisy-labels",
          "machine-learning",
          "moderator",
          "naive-bayes-classifier",
          "networks",
          "neural",
          "nsfw-data",
          "tensorflowjs",
          "vision-transformer",
          "yolo",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962409",
        "stars": 1,
        "forks": 0,
        "language": "Python",
        "topics": [
          "ai",
          "echarts",
          "google",
          "gradio",
          "learning-with-noisy-labels",
          "machine-learning",
          "moderator",
          "naive-bayes-classifier",
          "networks",
          "neural",
          "nsfw-data",
          "tensorflowjs",
          "vision-transformer",
          "yolo"
        ],
        "owner": "pedropqv",
        "created_at": "2025-05-12T12:44:33Z",
        "relevance_score": 4.501,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "zeroleng28/Multimodal-LLM-Powered-Chatbot",
        "url": "https://github.com/zeroleng28/Multimodal-LLM-Powered-Chatbot",
        "date": "2025-07-10T10:36:02Z",
        "summary": "A multimodal chatbot powered by LLMs capable of answering user queries based on text, images, PDFs, and audio input using Hugging Face and Streamlit.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "A multimodal chatbot powered by LLMs capable of answering user queries based on text, images, PDFs, and audio input using Hugging Face and Streamlit.",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911308",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "zeroleng28",
        "created_at": "2025-07-10T10:14:38Z",
        "relevance_score": 4.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "greatLiiAccount/vit-mini-explicit-content",
        "url": "https://github.com/greatLiiAccount/vit-mini-explicit-content",
        "date": "2025-07-12T08:10:45Z",
        "summary": "vit-mini-explicit-content is an image classification vision-language model fine-tuned from vit-base-patch16-224-in21k for a single-label classification task. It categorizes images based on their explicitness using the ViTForImageClassification architecture.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "vit-mini-explicit-content is an image classification vision-language model fine-tuned from vit-base-patch16-224-in21k for a single-label classification task. It categorizes images based on their explicitness using the ViTForImageClassification architecture.",
        "tags": [
          "github",
          "repository",
          "google",
          "gradio",
          "huggingface-transformers",
          "image-classification",
          "nsfw-classifier",
          "nsfw-detection",
          "nsfw-filter",
          "nsfw-recognition",
          "vision-transformer",
          "vit",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962350",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "google",
          "gradio",
          "huggingface-transformers",
          "image-classification",
          "nsfw-classifier",
          "nsfw-detection",
          "nsfw-filter",
          "nsfw-recognition",
          "vision-transformer",
          "vit"
        ],
        "owner": "greatLiiAccount",
        "created_at": "2025-05-22T02:48:32Z",
        "relevance_score": 4.5,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "MrAlonso9/Hand-Gesture-2-Robot",
        "url": "https://github.com/MrAlonso9/Hand-Gesture-2-Robot",
        "date": "2025-07-12T07:34:56Z",
        "summary": "Hand-Gesture-2-Robot is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to recognize hand gestures and map them to specific robot commands using the SiglipForImageClassification architecture.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Hand-Gesture-2-Robot is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to recognize hand gestures and map them to specific robot commands using the SiglipForImageClassification architecture.",
        "tags": [
          "github",
          "repository",
          "gesture-recognition",
          "huggingface-transformers",
          "image-classification",
          "jpeg",
          "pil",
          "pillow",
          "png",
          "robot",
          "siglip2",
          "vision-language-model",
          "vision-transformer",
          "visionprocessing",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962457",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "gesture-recognition",
          "huggingface-transformers",
          "image-classification",
          "jpeg",
          "pil",
          "pillow",
          "png",
          "robot",
          "siglip2",
          "vision-language-model",
          "vision-transformer",
          "visionprocessing"
        ],
        "owner": "MrAlonso9",
        "created_at": "2025-04-03T22:21:50Z",
        "relevance_score": 4.5,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "Rajadhopiya/Gender-Classifier-Mini",
        "url": "https://github.com/Rajadhopiya/Gender-Classifier-Mini",
        "date": "2025-07-12T07:32:42Z",
        "summary": "Gender-Classifier-Mini is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to classify images based on gender using the SiglipForImageClassification architecture. \n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Gender-Classifier-Mini is an image classification vision-language encoder model fine-tuned from google/siglip2-base-patch16-224 for a single-label classification task. It is designed to classify images based on gender using the SiglipForImageClassification architecture.",
        "tags": [
          "github",
          "repository",
          "gender-classification",
          "gender-detection",
          "gender-recognition",
          "gradio",
          "huggingface-transformers",
          "siglip",
          "siglip2",
          "vision-language-model",
          "vision-transformer",
          "vit",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962495",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [
          "gender-classification",
          "gender-detection",
          "gender-recognition",
          "gradio",
          "huggingface-transformers",
          "siglip",
          "siglip2",
          "vision-language-model",
          "vision-transformer",
          "vit"
        ],
        "owner": "Rajadhopiya",
        "created_at": "2025-04-01T07:35:11Z",
        "relevance_score": 4.5,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "swiss-ai/mmore",
        "url": "https://github.com/swiss-ai/mmore",
        "date": "2025-07-10T09:21:48Z",
        "summary": "Massive Multimodal Open RAG & Extraction  A scalable multimodal pipeline for processing, indexing, and querying multimodal documents  Ever needed to take 8000 PDFs, 2000 videos, and 500 spreadsheets and feed them to an LLM as a knowledge base? Well, MMORE is here to help you!\n\n‚≠ê 44 stars | üç¥ 17 forks | üìù Python",
        "content": "Massive Multimodal Open RAG & Extraction  A scalable multimodal pipeline for processing, indexing, and querying multimodal documents  Ever needed to take 8000 PDFs, 2000 videos, and 500 spreadsheets and feed them to an LLM as a knowledge base? Well, MMORE is here to help you!",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911341",
        "stars": 44,
        "forks": 17,
        "language": "Python",
        "topics": [],
        "owner": "swiss-ai",
        "created_at": "2024-11-27T00:45:54Z",
        "relevance_score": 3.714,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "isLinXu/paper-list",
        "url": "https://github.com/isLinXu/paper-list",
        "date": "2025-07-12T00:30:05Z",
        "summary": "autoupdate paper list\n\n‚≠ê 88 stars | üç¥ 10 forks | üìù Python",
        "content": "autoupdate paper list",
        "tags": [
          "github",
          "repository",
          "action-recognition",
          "anomaly-detection",
          "audio-processing",
          "classification",
          "depth-estimation",
          "graph-neural-networks",
          "image-generation",
          "llm",
          "multimodal",
          "object-detection",
          "object-tracking",
          "optical-flow",
          "pose-estimation",
          "reinforcement-learning",
          "scene-understanding",
          "semantic-segmentation",
          "transfer-learning",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910951",
        "stars": 88,
        "forks": 10,
        "language": "Python",
        "topics": [
          "action-recognition",
          "anomaly-detection",
          "audio-processing",
          "classification",
          "depth-estimation",
          "graph-neural-networks",
          "image-generation",
          "llm",
          "multimodal",
          "object-detection",
          "object-tracking",
          "optical-flow",
          "pose-estimation",
          "reinforcement-learning",
          "scene-understanding",
          "semantic-segmentation",
          "transfer-learning"
        ],
        "owner": "isLinXu",
        "created_at": "2024-02-29T14:43:43Z",
        "relevance_score": 3.688,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "automatika-robotics/embodied-agents",
        "url": "https://github.com/automatika-robotics/embodied-agents",
        "date": "2025-07-11T20:44:26Z",
        "summary": "EmbodiedAgents is a fully-loaded framework, written in pure ROS2, for creating interactive physical agents that can understand, remember, and act upon contextual information from their environment.\n\n‚≠ê 28 stars | üç¥ 2 forks | üìù Python",
        "content": "EmbodiedAgents is a fully-loaded framework, written in pure ROS2, for creating interactive physical agents that can understand, remember, and act upon contextual information from their environment.",
        "tags": [
          "github",
          "repository",
          "deeplearning",
          "embodied-agent",
          "embodied-ai",
          "generative-ai",
          "godel-machine",
          "llm",
          "machine-learning",
          "multimodal",
          "ollama",
          "physical-ai",
          "roboml",
          "robotics",
          "ros2",
          "vllm",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911324",
        "stars": 28,
        "forks": 2,
        "language": "Python",
        "topics": [
          "deeplearning",
          "embodied-agent",
          "embodied-ai",
          "generative-ai",
          "godel-machine",
          "llm",
          "machine-learning",
          "multimodal",
          "ollama",
          "physical-ai",
          "roboml",
          "robotics",
          "ros2",
          "vllm"
        ],
        "owner": "automatika-robotics",
        "created_at": "2023-09-05T16:11:25Z",
        "relevance_score": 3.548,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "path2generalist/General-Level",
        "url": "https://github.com/path2generalist/General-Level",
        "date": "2025-07-11T15:42:30Z",
        "summary": "On Path to Multimodal Generalist: General-Level and General-Bench\n\n‚≠ê 17 stars | üç¥ 2 forks | üìù Python",
        "content": "On Path to Multimodal Generalist: General-Level and General-Bench",
        "tags": [
          "github",
          "repository",
          "benchmark",
          "llm",
          "llm-evaluation",
          "mllm",
          "mllm-evaluation",
          "mllms",
          "multimodal-generalist",
          "multimodal-large-language-models",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911089",
        "stars": 17,
        "forks": 2,
        "language": "Python",
        "topics": [
          "benchmark",
          "llm",
          "llm-evaluation",
          "mllm",
          "mllm-evaluation",
          "mllms",
          "multimodal-generalist",
          "multimodal-large-language-models"
        ],
        "owner": "path2generalist",
        "created_at": "2024-06-15T15:36:07Z",
        "relevance_score": 3.537,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "willxxy/ECG-Bench",
        "url": "https://github.com/willxxy/ECG-Bench",
        "date": "2025-07-12T04:10:15Z",
        "summary": "A Unified Framework for Benchmarking Generative Electrocardiogram-Language Models (ELMs) \n\n‚≠ê 16 stars | üç¥ 2 forks | üìù Python",
        "content": "A Unified Framework for Benchmarking Generative Electrocardiogram-Language Models (ELMs)",
        "tags": [
          "github",
          "repository",
          "biosignals",
          "deep-learning",
          "ecg",
          "ecg-signal",
          "electrocardiogram",
          "generative-ai",
          "large-language-models",
          "llm",
          "machine-learning",
          "multimodal",
          "multimodal-deep-learning",
          "multimodal-large-language-models",
          "multimodal-learning",
          "physiological-signals",
          "signal-preprocessing",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910869",
        "stars": 16,
        "forks": 2,
        "language": "Python",
        "topics": [
          "biosignals",
          "deep-learning",
          "ecg",
          "ecg-signal",
          "electrocardiogram",
          "generative-ai",
          "large-language-models",
          "llm",
          "machine-learning",
          "multimodal",
          "multimodal-deep-learning",
          "multimodal-large-language-models",
          "multimodal-learning",
          "physiological-signals",
          "signal-preprocessing"
        ],
        "owner": "willxxy",
        "created_at": "2024-12-23T17:01:02Z",
        "relevance_score": 3.536,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "Lum1104/MER-Factory",
        "url": "https://github.com/Lum1104/MER-Factory",
        "date": "2025-07-11T15:25:39Z",
        "summary": "Your automated factory for constructing Multimodal Emotion Recognition and Reasoning (MERR) datasets.\n\n‚≠ê 9 stars | üç¥ 2 forks | üìù Python",
        "content": "Your automated factory for constructing Multimodal Emotion Recognition and Reasoning (MERR) datasets.",
        "tags": [
          "github",
          "repository",
          "affective-computing",
          "dataset-generator",
          "emotion-reasoning",
          "llm",
          "multimodal-emotion-datasets",
          "multimodal-emotion-recognition",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911123",
        "stars": 9,
        "forks": 2,
        "language": "Python",
        "topics": [
          "affective-computing",
          "dataset-generator",
          "emotion-reasoning",
          "llm",
          "multimodal-emotion-datasets",
          "multimodal-emotion-recognition"
        ],
        "owner": "Lum1104",
        "created_at": "2025-06-24T09:55:00Z",
        "relevance_score": 3.529,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "Niman1986/vlarl",
        "url": "https://github.com/Niman1986/vlarl",
        "date": "2025-07-12T07:42:41Z",
        "summary": "Single-file implementation to advance vision-language-action (VLA) models with reinforcement learning.\n\n‚≠ê 1 stars | üç¥ 1 forks | üìù Python",
        "content": "Single-file implementation to advance vision-language-action (VLA) models with reinforcement learning.",
        "tags": [
          "github",
          "repository",
          "github-config",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962438",
        "stars": 1,
        "forks": 1,
        "language": "Python",
        "topics": [
          "github-config"
        ],
        "owner": "Niman1986",
        "created_at": "2025-04-18T01:36:27Z",
        "relevance_score": 3.511,
        "relevance_reasons": [
          "Multimodal AI: vision-language",
          "Key Tech: reinforcement learning"
        ]
      },
      {
        "title": "multimodal-ai-lab/scrapeMM",
        "url": "https://github.com/multimodal-ai-lab/scrapeMM",
        "date": "2025-07-11T10:35:54Z",
        "summary": "LLM-friendly scraper for media and text from social media and the open web.\n\n‚≠ê 2 stars | üç¥ 0 forks | üìù Python",
        "content": "LLM-friendly scraper for media and text from social media and the open web.",
        "tags": [
          "github",
          "repository",
          "llm",
          "machine-learning",
          "media",
          "multimodal",
          "scrape",
          "scraper",
          "scraping-websites",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911147",
        "stars": 2,
        "forks": 0,
        "language": "Python",
        "topics": [
          "llm",
          "machine-learning",
          "media",
          "multimodal",
          "scrape",
          "scraper",
          "scraping-websites"
        ],
        "owner": "multimodal-ai-lab",
        "created_at": "2025-07-01T11:05:55Z",
        "relevance_score": 3.502,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "chrimage/shotty",
        "url": "https://github.com/chrimage/shotty",
        "date": "2025-07-11T22:42:23Z",
        "summary": "Screenshot MCP Server for GNOME Wayland - Window listing and capture tools for multimodal LLMs\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Screenshot MCP Server for GNOME Wayland - Window listing and capture tools for multimodal LLMs",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.910990",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "chrimage",
        "created_at": "2025-07-11T12:16:22Z",
        "relevance_score": 3.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "daslearning-org/AI-Web-Chatbot-Ollama",
        "url": "https://github.com/daslearning-org/AI-Web-Chatbot-Ollama",
        "date": "2025-07-11T13:27:23Z",
        "summary": "A web based AI Chatbot for Ollama made on Gradio (Python) which also supports Voice input to interact with ollama LLMs. It can work completely offline. We are also planning to make it multimodal for the LLMs which support Image input.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "A web based AI Chatbot for Ollama made on Gradio (Python) which also supports Voice input to interact with ollama LLMs. It can work completely offline. We are also planning to make it multimodal for the LLMs which support Image input.",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911106",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "daslearning-org",
        "created_at": "2025-07-11T05:38:10Z",
        "relevance_score": 3.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "KhushalM/background-multimodal-llm",
        "url": "https://github.com/KhushalM/background-multimodal-llm",
        "date": "2025-07-11T02:19:47Z",
        "summary": "Multimodal LLM Voice assistant with screen context\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "Multimodal LLM Voice assistant with screen context",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911182",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "KhushalM",
        "created_at": "2025-06-09T14:58:03Z",
        "relevance_score": 3.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "KaushikSakre/Learn-Mate",
        "url": "https://github.com/KaushikSakre/Learn-Mate",
        "date": "2025-07-10T18:32:32Z",
        "summary": "LearnMate is an AI-powered multimodal tutor that helps students understand science and math through diagrams and equations. It uses vision models, RAG, and Groq-hosted LLMs to deliver clear, curriculum-aligned answers. Built with Python, Django, React, and Docker.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "LearnMate is an AI-powered multimodal tutor that helps students understand science and math through diagrams and equations. It uses vision models, RAG, and Groq-hosted LLMs to deliver clear, curriculum-aligned answers. Built with Python, Django, React, and Docker.",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:54.911233",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "KaushikSakre",
        "created_at": "2025-07-02T09:41:59Z",
        "relevance_score": 3.5,
        "relevance_reasons": [
          "Multimodal AI: multimodal",
          "Key Tech: llm"
        ]
      },
      {
        "title": "Minhaj-21st/asl-vit",
        "url": "https://github.com/Minhaj-21st/asl-vit",
        "date": "2025-07-12T02:06:56Z",
        "summary": "A Vision Transformer model for classifying A‚ÄìZ American Sign Language gestures.\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "A Vision Transformer model for classifying A‚ÄìZ American Sign Language gestures.",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962612",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "Minhaj-21st",
        "created_at": "2025-06-21T22:11:15Z",
        "relevance_score": 3.5,
        "relevance_reasons": [
          "Multimodal AI: vision transformer",
          "Key Tech: transformer"
        ]
      },
      {
        "title": "hrlics/HoPE",
        "url": "https://github.com/hrlics/HoPE",
        "date": "2025-07-12T01:02:27Z",
        "summary": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models\n\n‚≠ê 11 stars | üç¥ 1 forks | üìù Python",
        "content": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models",
        "tags": [
          "github",
          "repository",
          "long-context-modeling",
          "long-video-understanding",
          "position-embedding",
          "vision-language-models",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962637",
        "stars": 11,
        "forks": 1,
        "language": "Python",
        "topics": [
          "long-context-modeling",
          "long-video-understanding",
          "position-embedding",
          "vision-language-models"
        ],
        "owner": "hrlics",
        "created_at": "2025-05-26T17:40:48Z",
        "relevance_score": 2.021,
        "relevance_reasons": [
          "Multimodal AI: vision-language"
        ]
      },
      {
        "title": "ai4ce/INT-ACT",
        "url": "https://github.com/ai4ce/INT-ACT",
        "date": "2025-07-11T15:19:45Z",
        "summary": "Official repo for From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models\n\n‚≠ê 19 stars | üç¥ 0 forks | üìù Python",
        "content": "Official repo for From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
        "tags": [
          "github",
          "repository",
          "benchmarking",
          "evaluation",
          "vision-language-action",
          "vision-language-action-model",
          "vla",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962755",
        "stars": 19,
        "forks": 0,
        "language": "Python",
        "topics": [
          "benchmarking",
          "evaluation",
          "vision-language-action",
          "vision-language-action-model",
          "vla"
        ],
        "owner": "ai4ce",
        "created_at": "2025-06-05T19:58:56Z",
        "relevance_score": 2.019,
        "relevance_reasons": [
          "Multimodal AI: vision-language"
        ]
      },
      {
        "title": "panchaldhruv27223/Vision-Language-Model",
        "url": "https://github.com/panchaldhruv27223/Vision-Language-Model",
        "date": "2025-07-11T16:31:14Z",
        "summary": "‚≠ê 1 stars | üç¥ 0 forks | üìù Python",
        "content": "",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962726",
        "stars": 1,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "panchaldhruv27223",
        "created_at": "2025-04-14T05:39:06Z",
        "relevance_score": 2.001,
        "relevance_reasons": [
          "Multimodal AI: vision-language"
        ]
      },
      {
        "title": "ZhuWenjie98/KRNFT",
        "url": "https://github.com/ZhuWenjie98/KRNFT",
        "date": "2025-07-12T08:19:14Z",
        "summary": "ACM MM 2025 Knowledge Regularized Negative Feature Tuning for Out-of-Distribution Detection with Vision-Language Models\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "ACM MM 2025 Knowledge Regularized Negative Feature Tuning for Out-of-Distribution Detection with Vision-Language Models",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962530",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "ZhuWenjie98",
        "created_at": "2025-07-12T06:00:50Z",
        "relevance_score": 2.0,
        "relevance_reasons": [
          "Multimodal AI: vision-language"
        ]
      },
      {
        "title": "Zhenhan-Huang/Differentiable-Prompt-Learn",
        "url": "https://github.com/Zhenhan-Huang/Differentiable-Prompt-Learn",
        "date": "2025-07-11T19:11:39Z",
        "summary": "[IJCAI'25] Differentiable Prompt Learning for Vision Language Models\n\n‚≠ê 0 stars | üç¥ 0 forks | üìù Python",
        "content": "[IJCAI'25] Differentiable Prompt Learning for Vision Language Models",
        "tags": [
          "github",
          "repository",
          "python"
        ],
        "source": "GitHub",
        "crawled_at": "2025-07-12T16:31:56.962707",
        "stars": 0,
        "forks": 0,
        "language": "Python",
        "topics": [],
        "owner": "Zhenhan-Huang",
        "created_at": "2025-04-30T00:06:02Z",
        "relevance_score": 2.0,
        "relevance_reasons": [
          "Multimodal AI: vision language model"
        ]
      }
    ]
  }
}